<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Docker 配置 MPI 并行环境并实现矩阵乘并行 | BouseBlog</title>
<link rel="shortcut icon" href="https://bouse-w.github.io/BouseBlog.github.io//favicon.ico?v=1742972353564">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://bouse-w.github.io/BouseBlog.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Docker 配置 MPI 并行环境并实现矩阵乘并行 | BouseBlog - Atom Feed" href="https://bouse-w.github.io/BouseBlog.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="参考文章

基于 docker 的 MPI 安装 | CSDN | blog.csdn.net
并行程序设计 MPI实现矩阵乘法（按行并行，分块并行，Cannon卡农算法）| CSDN | blog.csdn.net

MPI-API Re..." />
    <meta name="keywords" content="HPC" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://bouse-w.github.io/BouseBlog.github.io/">
  <img class="avatar" src="https://bouse-w.github.io/BouseBlog.github.io//images/avatar.png?v=1742972353564" alt="">
  </a>
  <h1 class="site-title">
    BouseBlog
  </h1>
  <p class="site-description">
    让我们重新开始
  </p>
  <div class="menu-container">
    
      
        <a href="https://bouse-w.github.io/BouseBlog.github.io/" class="menu">
          首页
        </a>
      
    
      
        <a href="https://bouse-w.github.io/BouseBlog.github.io/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="https://bouse-w.github.io/BouseBlog.github.io/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="https://bouse-w.github.io/BouseBlog.github.io/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Docker 配置 MPI 并行环境并实现矩阵乘并行
            </h2>
            <div class="post-info">
              <span>
                2025-03-18
              </span>
              <span>
                15 min read
              </span>
              
                <a href="https://bouse-w.github.io/BouseBlog.github.io/tag/FtDeh3RFRO/" class="post-tag">
                  # HPC
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <h5 id="参考文章">参考文章</h5>
<ul>
<li><a href="https://blog.csdn.net/weixin_50674454/article/details/124770487">基于 docker 的 MPI 安装 | CSDN | blog.csdn.net</a></li>
<li><a href="https://blog.csdn.net/WinterShiver/article/details/88987484">并行程序设计 MPI实现矩阵乘法（按行并行，分块并行，Cannon卡农算法）| CSDN | blog.csdn.net</a></li>
</ul>
<h5 id="mpi-api-references">MPI-API References</h5>
<ul>
<li>请参见 <a href="https://learn.microsoft.com/en-us/message-passing-interface/">Microsoft 提供的 API 开放文档-Microsoft MPI | Microsoft | learn.microsoft.com</a></li>
</ul>
<h5 id="环境配置过程">环境配置过程</h5>
<ul>
<li>
<p>配置 <code>Dockerfile</code> 构建 <code>mpich3.31</code> 的系统镜像，详细内容请参见 <a href="http://rerevolution.cn/2023/12/18/mpich-on-debian-bullseye/">Mpich on Debian-Bullseye | BouseBlog | rerevolution.cn</a></p>
<ul>
<li>在 <code>Dockerfile</code> 同级目录下执行</li>
</ul>
<pre><code class="language-shell">docker build -t &lt;container_name&gt;:&lt;container_version&gt; .
</code></pre>
</li>
<li>
<p>在宿主机和镜像中各新建一个 <code>mpiWorkspace/</code> 目录并挂载一处，这样所有容器可以共享工作区文件，无需在节点中使用 <code>scp</code> 命令等在各节点上复制多份分别保存，尽管在真实的多机并行环境下我们是这么做的。</p>
<ul>
<li>镜像中的 <code>mpiWorkspace</code> 访问路径为 <code>/home/mpi/workspace/</code>，具体请见我的 <code>Dockerfile</code>。</li>
</ul>
</li>
<li>
<p>写了一个简陋的 <code>Shell</code> 脚本集成“用 <code>Docker</code> 新建网段”、“在指定网段下运行指定数量的容器”等几个操作。哥们脚本写得太菜了，请谅见。</p>
</li>
</ul>
<pre><code class="language-shell">	#!/bin/bash

	HOSTS_NUM=3

	#  新建网段
	docker network create --subnet=192.168.10.0/16 mpi_network

	# 以指定 IP 启动容器，挂载本地目录
	docker run -it --name node0 -h node0 
		--net mpi_network --ip 192.168.10.20 
		--add-host node1:192.168.10.21 
		--add-host node2:192.168.10.22 
		-v /home/trash-w/mpiWorkspace:/home/mpi/workspace 
		mpi_ch331:v0 /bin/bash

	docker run -it --name node1 -h node1 
		--net mpi_network --ip 192.168.10.21 
		--add-host node0:192.168.10.20 
		--add-host node2:192.168.10.22 
		-v /home/trash-w/mpiWorkspace:/home/mpi/workspace 
		mpi_ch331:v0 /bin/bash

	docker run -it --name node2 -h node2 
		--net mpi_network --ip 192.168.10.22 
		--add-host node1:192.168.10.21 
		--add-host node0:192.168.10.20 
		-v /home/trash-w/mpiWorkspace:/home/mpi/workspace 
		mpi_ch331:v0 /bin/bash
</code></pre>
<ul>
<li>
<p>进入容器的伪终端，启动 <code>sshd</code>、切换到本地用户、使用 <code>ssh-keygen</code> 命令生成 <code>ssh</code> 公钥、使用 <code>ssh-copy-id</code> 命令分发公钥配置节点间免密登录。</p>
<ul>
<li>执行</li>
</ul>
<pre><code class="language-shell">/usr/sbin/sshd -D &amp;
</code></pre>
<ul>
<li>执行</li>
</ul>
<pre><code class="language-shell">su - localuser
</code></pre>
<p>用户 “<code>localuser</code>” 在 <code>build</code> 镜像时已经创建并加入了 <code>root</code> 用户组。</p>
<ul>
<li>
<p>之所以不直接使用 <code>root</code> 用户是因为辛辛苦苦在镜像里创建了 <code>localuser</code> 不用白不用嘛（</p>
<ul>
<li>是因为不知道 <code>docker</code> 镜像中 <code>root</code> 用户的默认密码，镜像文件写死了好像也无法修改，于是无法使用 <code>ssh-cpy-id</code> 正常分发公钥。干脆就在镜像中自建一个 <code>localuser</code>，密码设置为 <code>123456</code>，具体可见我的 <code>Dockerfile</code>。</li>
</ul>
</li>
<li>
<p>执行</p>
</li>
</ul>
<pre><code class="language-shell">ssh-keygen -t rsa -b 4096
</code></pre>
<p>一路回车即可。或者按你钟意的参数生成公钥。</p>
<ul>
<li>执行</li>
</ul>
<pre><code class="language-shell">ssh-copy-id localuser@node0
ssh-copy-id localuser@node1
...
</code></pre>
<p>这是向指定节点的指定用户分享公钥以实现免密登录。</p>
<ul>
<li>保险起见，要向包括自己在内的所有用户分享公钥。方便起见，所有节点上的自建用户都应当取相同的用户名。</li>
<li>由于几个节点都是在 <code>Docker</code> 创建的同一网段下运行的容器，所以相互间可以直接将节点名解析为 <code>IP</code> 地址，不用再手动向 <code>/etc/hosts</code> 文件添加内容。这和真实的多机并行环境非常不同。</li>
</ul>
</li>
</ul>
<h5 id="hpc-lab4-矩阵相乘使用-mpi-并行">HPC-lab4-矩阵相乘使用 MPI 并行</h5>
<ul>
<li>
<p>主要的代码框架</p>
<ul>
<li><code>main</code> 函数</li>
</ul>
<pre><code class="language-cpp">#include &lt;math.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/time.h&gt;
#include &lt;iostream&gt;
#include &quot;mpi.h&quot;
using namespace std;
int main (int argc, char * argv[])
{
		if (4 != argc)
		{
			cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot;M N Kn&quot;;
			exit (-1);
		}

		int rank;
		int worldsize;

		/* 获取命令行的执行参数，初始化 MPI */
		MPI_Init (&amp;argc, &amp;argv);

		/* 设置 MPI 通信域大小 */
		MPI_Comm_size (MPI_COMM_WORLD, &amp;worldsize);

		/* 设置进程标记变量，每个进程私有一个 &quot;rank&quot; */
		MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);

		/* 设置矩阵 A[m * n]，B[n * k] 的规模，本例中我们使用 2 个同等规模的方阵 */
		int m = atoi(argv[1]);
		int n = atoi(argv[2]);
		int k = atoi(argv[3]);

		float * leftMat;
		float * rightMat;
		float * resMat;

		/* 时间结构体变量 */
		struct timeval start, stop;

		/**
		 * 为乘数矩阵和结果矩阵申请空间并初始化
		 * 这里条件判断 &quot;rank&quot; 使矩阵初始化只在主进程进行
		 */
		if (0 == rank)
		{
			randMat (m, n, leftMat);
			randMat (n, k, rightMat);
			randMat (m, k, resMat);
		}

		gettimeofday (&amp;start, NULL);

		/* MPI 并行计算矩阵乘 */
		mpi_sgemm (m, n, k, leftMat, rightMat, resMat, rank, worldsize);

		gettimeofday (&amp;stop, NULL);

		if (rank == 0)
		{
			cout &lt;&lt; &quot;mpi matmul: &quot;
				&lt;&lt; (stop.tv_sec - start.tv_sec) * 1000.0 +
					(stop.tv_usec -start.tv_usec) / 1000.0
				&lt;&lt; &quot; ms&quot; &lt;&lt; endl;

			/* 检验计算结果的数值正确性 */
			for (int i = 0; i &lt; m; i++)
			{
				for (int j = 0; j &lt; k; j++)
				{
					if (int (resMat[i * k + j]) != n)
					{
						cout &lt;&lt; resMat[i * k + j] &lt;&lt; &quot;errorn&quot;;
						exit (-1);
					}
					//cout &lt;&lt; resMat[i * k + j] &lt;&lt; ' ';
				}
				//cout &lt;&lt; endl;
			}
		}

		/* 结束运行 MPI 并行程序 */
		MPI_Finalize ();
}
</code></pre>
<ul>
<li>
<p>一点简单说明</p>
<ul>
<li>方便起见，我们取乘矩阵为两个规模相同的方阵，结果矩阵自然保持一致。</li>
<li>乘矩阵规模假设为 [latex]n \times n[/latex]，其中所有元素均初始化为 <code>1</code>，则结果矩阵规模为 [latex]n \times n[/latex]，所有元素值为 <code>n</code></li>
</ul>
</li>
<li>
<p>调用到的标准 <code>MPI-API</code>，下皆摘自 <code>Microsoft MPI</code></p>
<pre><code class="language-cpp">/**
 * MPI_Init
 * Initializes the calling MPI process’s execution environment for single threaded execution.
 */
int MPIAPI MPI_Init (_In_opt_ int *argc,
					_In_opt_count_(*argc) char ***argv);

/**
 * MPI_Comm_size
 * Retrieves the number of processes involved in a communicator, or the total number of processes available.
 */
int MPIAPI MPI_Comm_size (MPI_Comm comm, _Out_ int *size);

/**
 * MPI_Comm_rank
 * Retrieves the rank of the calling process in the group of the specified communicator.
 */
int MPIAPI MPI_Comm_rank (MPI_Comm comm, _Out_ int *rank );

/**
 * MPI_Finalize
 * Terminates the calling MPI process’s execution environment.
 */
int MPIAPI MPI_Finalize (void);
</code></pre>
</li>
<li>
<p><code>randMat</code> 函数</p>
</li>
</ul>
<pre><code class="language-cpp">/**
	* @param rows 矩阵行数
	* @param cols 矩阵列数
	* @param Mat 矩阵地址
	* 随机填充矩阵（初始化）
	*/
void randMat (int rows, int cols, float *&amp; Mat)
{
	Mat = new float[rows * cols];

	for (int i = 0; i &lt; rows; i++)
	{
		for (int j = 0; j &lt; cols; j++)
		{
			Mat[i * cols + j] = 1.0;
		}
	}
}
</code></pre>
<ul>
<li>
<p>本篇代码使用了 <code>CUDA-C++</code>，故出现了一些 <code>C++</code> 的专有语法，比如 <code>float *&amp; Mat;</code></p>
<ul>
<li><code>float *&amp; Mat;</code> 定义了一个指向浮点变量的<strong>特殊指针</strong> <code>Mat</code>。<code>Mat</code> <strong>既是一级指针</strong>：直接指向一般类型变量，使用下标或解引用符访问所指地址的内容，<strong>又是二级指针</strong>：可以直接给它赋新指针值，修改它所指一级指针指向的地址。<pre><code class="language-cpp">/* C 语言写法 */
float * array_a;
float ** p_array_a = &amp;array_a;
*p_array_a = (float *)malloc (sizeof (float) * 10);
*p_array_a[0] = 10;

/* C++ 写法 */
float * array_a;
float *&amp; p_array_a = array_a;
p_array_a = new float[10];
p_array_a[0] = 10;
</code></pre>
</li>
</ul>
</li>
<li>
<p><code>mpi_sgemm</code> 函数</p>
</li>
</ul>
<pre><code class="language-cpp">void mpi_sgemm (int m, int n, int k,
					float *&amp; leftMat, float *&amp; rightMat, float *&amp; resultMat,
					int rank, int worldsize)
{
		int rowBlock = sqrt (worldsize);

		if (rowBlock * rowBlock &gt; worldsize)
		{
			rowBlock -= 1;
		}

		int colBlock = rowBlock;

		int rowStride = m / rowBlock;
		int colStride = k / colBlock;

		/**
		 * Re-calculate the number of mpi-process
		 * We abandon some processes, so best set process to a square number
		 */
		worldsize = rowBlock * colBlock;

		/**
		 * In slave processes: save the partial result
		 * In masert processes: recieve and put together
		 */
		float * res;

		/**
		 * The master process, transpose the right Mat
		 */
		if (rank == 0)
		{
			float * buf = new float[k * n];

			/**
			 * TODO Improvement 1: Transpose rightMat with openmp
			 */
			for (int r = 0; r &lt; n; r++)
			{
				for (int c = 0; c &lt; k; c++)
				{
					buf[c * n + r] = rightMat[r * k + c];
				}
			}

			for (int r = 0; r &lt; k; r++)
			{
				for (int c = 0; c &lt; n; c++)
				{
					rightMat[r * n + c] = buf[r * n + c];
				}
			}

			delete buf;

			/**
			 * Master-Slave Mode, send sub-matrix to sub-processes
			 */
			MPI_Request sendRequest[2 * worldsize];
			MPI_Status status[2 * worldsize];
			int surplusRow = 0, surplusCol = 0;

			for (int rowB = 0; rowB &lt; rowBlock; rowB++)
			{
				surplusRow = 0;
				if ((0 != (m % rowBlock)) &amp;&amp; ((rowBlock - 1) == rowB))
				{
					surplusRow = (m % rowBlock);
				}
				for (int colB = 0; colB &lt; colBlock; colB++)
				{
					surplusCol = 0;
					/**
					 * TODO: Calculate the size of a block
					 */
					rowStride = m / rowBlock;
					colStride = k / colBlock;

					/**
					 * Can't divide the matrix by whole
					 * Square matrix only !
					 */
					if ((0 != (k % colBlock)) &amp;&amp; ((colBlock - 1) == colB))
					{
						surplusCol = (k % colBlock);
					}

					/**
					 * The rank of target process
					 */
					int sendTo = rowB * colBlock + colB;

					if (sendTo == 0)
					{
						continue;
					}

					/**
					 * TODO: Send data with no-blocking MPI_ISend
					 */
					MPI_Isend (&amp;leftMat[rowB * rowStride], (rowStride + surplusRow) * n, MPI_FLOAT, sendTo, sendTo, MPI_COMM_WORLD, &amp;sendRequest[sendTo]);
					MPI_Isend (&amp;rightMat[colB * colStride], (colStride + surplusCol) * n, MPI_FLOAT, sendTo, sendTo, MPI_COMM_WORLD, &amp;sendRequest[worldsize + sendTo]);
				}
			}

			/**
			 * Wait for data to be send with MPI_Wait ()
			 */
			for (int rowB = 0; rowB &lt; rowBlock; rowB++)
			{
				for (int colB = 0; colB &lt; colBlock; colB++)
				{
					int recvFrom = rowB * colBlock + colB;

					if (recvFrom == 0)
					{
						continue;
					}

					MPI_Wait (&amp;sendRequest[recvFrom], &amp;status[recvFrom]);
					MPI_Wait (&amp;sendRequest[worldsize + recvFrom], &amp;status[worldsize + recvFrom]);
				}
			}

			res = new float[(m / rowBlock) * (k / colBlock)];
		}
		else
		{
			/**
			 * Receive data send by the master
			 */
			if (rank &lt; worldsize)
			{
				MPI_Status status[2];

				/**
				 * TODO: Calculate the size of stride-like matrix for the current rank
				 */
				rowStride = m / rowBlock;
				colStride = k / colBlock;

				/**
				 * Handle surplus colume
				 */
				if ((0 == ((worldsize - 1 - rank) % colBlock))
					&amp;&amp; (0 != (k % colBlock)))
				{
					colStride += (k % colBlock);
				}

				/**
				 * Handle surplus row
				 */
				if (((rank &lt;= (worldsize - 1))
					&amp;&amp; (rank &gt;= (worldsize - rowBlock)))
					&amp;&amp; (0 != (m % rowBlock)))
				{
					rowStride += (m % rowBlock);
				}

				if (0 != rank)
				{
					leftMat = new float[rowStride * n];
					rightMat = new float[colStride * n];
				}

				if (0 != rank)
				{
					/**
					 * TODO: Receive data with MPI_Recv
					 */
					MPI_Recv (leftMat, rowStride * n, MPI_FLOAT, 0, rank, MPI::COMM_WORLD, &amp;status[0]);
					MPI_Recv (rightMat, colStride * n, MPI_FLOAT, 0, rank, MPI::COMM_WORLD, &amp;status[1]);
				}

				res = new float[rowStride * colStride];
			}
		}

		MPI_Barrier (MPI_COMM_WORLD);

		/**
		 * Local sub-matrix times others
		 * The master should take part in the computation too
		 */
		if (rank &lt; worldsize)
		{
			/**
			 * TODO: Calculate the size of stride-like matrix for the currnet rank
			 */

			/**
			 * Call openmp function
			 */
			openmp_sgemm (rowStride, n, colStride, leftMat, rightMat, res);
		}
		MPI_Barrier (MPI_COMM_WORLD);

		/**
		 * TODO: Sub-processes send the master result matrixes
		 */
		if (0 == rank)
		{
			MPI_Status status[worldsize];
			MPI_Request request[worldsize];
			int surplusRow = 0, surplusCol = 0;

			for (int rowB = 0; rowB &lt; rowBlock; rowB++)
			{
				surplusRow = 0;
				if ((0 != (m % rowBlock)) &amp;&amp; ((rowBlock - 1) == rowB))
				{
					surplusRow = (m % rowBlock);
				}
				for (int colB = 0; colB &lt; colBlock; colB++)
				{
					surplusCol = 0;

					rowStride = m / rowBlock;
					colStride = k / colBlock;

					if ((0 != (k % colBlock)) &amp;&amp; ((colBlock - 1) == colB))
					{
						surplusCol = (k % colBlock);
					}

					int recvFrom = rowB * colBlock + colB;
					float * recvBuffer;

					if (0 == recvFrom)
					{
						recvBuffer = res;
					}
					else
					{
						recvBuffer = new float[(rowStride + surplusRow) * (colStride + surplusCol)];
						//MPI_Recv (recvBuffer, ((rowStride + surplusRow) * (colStride + surplusCol)), MPI_FLOAT, recvFrom, recvFrom, MPI::COMM_WORLD, &amp;status[recvFrom]);
						MPI_Irecv (recvBuffer, ((rowStride + surplusRow) * (colStride + surplusCol)), MPI_FLOAT, recvFrom, recvFrom, MPI::COMM_WORLD, &amp;request[recvFrom]);
					}

					/**
					 * TODO Improvement 0: Fill up resultMat with openmp
					 */
					for (int i = 0; i &lt; (rowStride + surplusRow); i++)
					{
						for (int j = 0; j &lt; (colStride + surplusCol); j++)
						{
							resultMat[(rowB * rowStride + i) * n + (colB * colStride + j)] = recvBuffer[i * (colStride + surplusCol) + j];
							//cout &lt;&lt; resultMat[(rowB * rowStride + i) * n + (colB * colStride + j)] &lt;&lt; &quot; &quot;;
						}
						//cout &lt;&lt; endl;
					}

					/*
					for (int i = 0; i &lt; (rowStride + surplusRow); i++)
					{
						for (int j = 0; j &lt; (colStride + surplusCol); j++)
						{
							cout &lt;&lt; recvBuffer[i * (colStride + surplusCol) + j] &lt;&lt; &quot; &quot;;
						}
						cout &lt;&lt; &quot;recvFrom &quot; &lt;&lt; recvFrom &lt;&lt; endl;
					}
					*/

					delete recvBuffer;
				}
			}

			/*
			for (int i = 0; i &lt; rowStride; i++)
			{
				for (int j = 0; j &lt; colStride; j++)
				{
					cout &lt;&lt; resultMat[i * colStride + j] &lt;&lt; &quot; &quot;;
				}
				cout &lt;&lt; &quot;rank: &quot; &lt;&lt; rank &lt;&lt; &quot; &quot; &lt;&lt; endl;
			}
			*/

		}
		else
		{
			/*
			MPI_Request sendRequest;
			MPI_Status status;
			*/
			if (rank &lt; worldsize)
			{
				/*
				MPI_Isend (res, rowStride * colStride, MPI_FLOAT, 0, rank, MPI::COMM_WORLD, &amp;sendRequest);
				MPI_Wait (&amp;sendRequest, &amp;status);
				*/
				MPI_Send (res, rowStride * colStride, MPI_FLOAT, 0, rank, MPI::COMM_WORLD);
			}
		}

		MPI_Barrier (MPI_COMM_WORLD);

		return;
	}
</code></pre>
<ul>
<li>有一点冗长，坚持一下就能读完。编写用意可以查看注释。</li>
<li><code>mpi_sgemm</code> 函数是使用 <code>MPI</code> 方式以<strong>主从模式</strong>计算矩阵相乘的核心代码。主进程 <code>rank0</code> 也负责一部分运算。矩阵划分、发送方式见以下图示</li>
</ul>
	<div align=center>
		<img src="/wp-content/uploads/2024/02/矩阵划分方式.jpg" alt="automatic-scalability" width=80%>
	</div>
<ul>
<li>这个函数里可以看出：在 <code>MPI</code> 程序中，根据进程编号不同使用条件语句控制不同进程运行不同代码是<strong>实现 <code>MPMD</code> 模型</strong>的一种可行办法。</li>
</ul>
<pre><code class="language-cpp">...
/* 使通信域中每个进程都创建私有变量 &quot;rank&quot; 保存进程编号 */
MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank)
...
/* 主进程 */
if (0 == rank)
{
	...
}
/* 子进程 */
else if (0 != rank)
{
	...
}
...
</code></pre>
<ul>
<li>
<p>调用到的标准 <code>MPI-API</code>，下皆摘自 <code>Microsoft MPI</code></p>
<pre><code class="language-cpp">/**
 * MPI_Isend
 * Initiates a standard mode send operation and returns a handle to the requested communication operation
 */
int MPIAPI MPI_Isend (_In_opt_ void * buf, int count,
						MPI_Datatype datatype,
						int dest, int tag,
						MPI_Comm comm,
						_Out_ MPI_Request * request);

/**
 * MPI_Wait
 * Completes an outstanding operation
 */
int MPIAPI MPI_Wait (_Inout_ MPI_Request * request,
						_Out_ MPI_Status * status);

/**
 * MPI_Recv
 * Performs a receive operation and does not return until a matching message is received
 */
int MPIAPI MPI_Recv (_In_opt_ void * buf, int count,
						MPI_Datatype datatype,
						int source, int tag,
						MPI_Comm comm,
						_Out_ MPI_Status * status);

/**
 * MPI_Barrier
 * Initiates barrier synchronization across all members of a group
 */
int MPIAPI MPI_Barrier (_In_ MPI_Comm comm);

/**
 * MPI_Irecv
 * Initiates a receive operation and returns a handle to the requested communication operation
 */
int MPIAPI MPI_Irecv (_In_opt_ void * buf, int count,
						MPI_Datatype datatype,
						int source, int tag,
						MPI_Comm comm,
						_Out_ MPI_Request * request);

/**
 * MPI_Send
 * Performs a standard mode send operation and returns when the send buffer can be safely reused
 */
int MPIAPI MPI_Send (_In_opt_ void * buf, int count,
						MPI_Datatype datatype,
						int dest, int tag,
						MPI_Comm comm);
</code></pre>
</li>
<li>
<p><code>openmp_sgemm</code> 函数</p>
</li>
</ul>
<pre><code class="language-cpp">void openmp_sgemm (int m, int n, int k,
				float *&amp; leftMat, float *&amp; rightMat, float *&amp; resultMat)
{
/**
 * TODO: use openmp
 * Right Mat is already transposed
 * Can be accelerated with openmp
 */
 /* 外层指定 4 线程并行，实际上削弱了加速效果 */
#pragma omp parallel num_threads(4)
{
	int row = 0, col = 0, i = 0;

	#pragma omp for
	for (row = 0; row &lt; m; row++)
	{
		for (col = 0; col &lt; k; col++)
		{
			resultMat[row * k + col] = 0.0;

			for (i = 0; i &lt; n; i++)
			{
				resultMat[row * k + col] += leftMat[row * n + i] * rightMat[col * n + i];
			}
		}
	}
}

	return;
}
</code></pre>
<ul>
<li><code>openmp_sgemm</code> 函数是计算分块矩阵相乘的代码，使用了 <code>openmp</code> 实现多线程加速。由于线程调度会带来一定开销，所以并不是并行线程越多加速效果越好。实际上，使用本文中的方法计算矩阵相乘，问题表现出弱可扩展性。</li>
</ul>
</li>
</ul>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0">参考文章</a></li>
<li><a href="#mpi-api-references">MPI-API References</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B">环境配置过程</a></li>
<li><a href="#hpc-lab4-%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E4%BD%BF%E7%94%A8-mpi-%E5%B9%B6%E8%A1%8C">HPC-lab4-矩阵相乘使用 MPI 并行</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://bouse-w.github.io/BouseBlog.github.io/post/zhan-yong-da-kuai-nei-cun-de-dui-xiang-zai-dui-he-zhan-shang-de-zheng-que-fang-zhi/">
              <h3 class="post-title">
                占用大块内存的对象在堆和栈上的正确放置
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  <p><font size=1>原博客网站 rerevolution.cn（服务器过期了）的重建</font></p>
  <a class="rss" href="https://bouse-w.github.io/BouseBlog.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
